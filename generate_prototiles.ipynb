{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json, math, os, struct, sys\n",
    "from functools import cache\n",
    "import threading\n",
    "from psql_utils.epsql import Engine\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from tempfile import NamedTemporaryFile\n",
    "from tqdm.notebook import tqdm\n",
    "from utils.utils import SimpleThreadPoolExecutor, Stopwatch, subprocess_check\n",
    "\n",
    "@cache\n",
    "def cached_engine(pid, tid):\n",
    "    return Engine()\n",
    "\n",
    "def engine():\n",
    "    return cached_engine(os.getpid(), threading.get_ident())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geoids_2010 = json.load(open('block_geoids_2010.json'))\n",
    "# Read block_geoids_2020.json.gz into geoids_2020\n",
    "geoids_2020 = json.load(gzip.open('block_geoids_2020.json.gz'))\n",
    "# print(\"Geoids in 2010:\", len(geoids_2010))\n",
    "print(\"Geoids in 2020:\", len(geoids_2020))\n",
    "\n",
    "geoid_to_idx = {geoid: idx+1 for idx, geoid in enumerate(geoids_2020)}\n",
    "idx_to_geoid = {idx+1: geoid for idx, geoid in enumerate(geoids_2020)}\n",
    "\n",
    "# counties_2020 = sorted({geoid[:5] for geoid in geoids_2020})\n",
    "# print(\"Counties in 2020:\", len(counties_2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine().execute(\"create schema if not exists dotmaps\")\n",
    "#engine().execute(\"drop table if exists dotmaps.geoids_2020\")\n",
    "if not engine().table_exists('dotmaps.geoids_2020'):\n",
    "    df = pd.DataFrame({'idx': geoid_to_idx.values(), 'geoid': geoid_to_idx.keys()})\n",
    "    df.to_sql('geoids_2020', engine().engine, 'dotmaps', if_exists='replace', index=False)\n",
    "    # primary key is geoid\n",
    "    engine().execute(\"alter table dotmaps.geoids_2020 add primary key (geoid)\")\n",
    "    # Add unique index on idx\n",
    "    engine().execute(\"create unique index geoids_2020_idx on dotmaps.geoids_2020 (idx)\")\n",
    "    print(\"Created table dotmaps.geoids_2020\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = {\n",
    "    1990: \"old_block2020/census1990_block2010-P0010001.2020.float32\",\n",
    "    2000: \"old_block2020/census2000_block2010-P0010001.2020.float32\",\n",
    "    2010: \"old_block2020/census2010_block2010-P0010001.2020.float32\",\n",
    "    2020: \"census2020_block2020/P0010001.2020.float32\"\n",
    "}\n",
    "\n",
    "maxpop = None\n",
    "\n",
    "print(\"Note that years before 2020 may be missing Puerto Rico.\")\n",
    "for year, path in years.items():\n",
    "    pop = np.fromfile(path, dtype=np.float32)\n",
    "    #print(year, pop[:10])\n",
    "    print(f\"Year {year}: Population {pop.sum():,.0f}\")\n",
    "    if maxpop is None:\n",
    "        maxpop = pop\n",
    "    else:\n",
    "        maxpop = np.maximum(maxpop, pop)\n",
    "\n",
    "maxdots = np.ceil(maxpop * 2 + 60).astype(np.int32)\n",
    "maxdots[0] = 0\n",
    "cumulative_dots = np.cumsum(maxdots, dtype=np.int64)\n",
    "ndots = cumulative_dots[-1]\n",
    "assert ndots == maxdots.sum()\n",
    "print(f\"Total dots: {cumulative_dots[-1]:,.0f}\")\n",
    "\n",
    "engine().execute(\"create schema if not exists dotmaps\")\n",
    "#engine().execute(\"drop table if exists dotmaps.prototile_2020_dotcount\")\n",
    "if not engine().table_exists('dotmaps.prototile_2020_dotcount'):\n",
    "    df = pd.DataFrame({'idx': range(1, len(maxdots)),\n",
    "                       'dotcount': maxdots[1:]})\n",
    "    df.to_sql('prototile_2020_dotcount', engine().engine, 'dotmaps', if_exists='replace', index=False)\n",
    "    # primary key is idx\n",
    "    engine().execute(\"alter table dotmaps.prototile_2020_dotcount add primary key (idx)\")\n",
    "    print(\"Created table dotmaps.prototile_2020_dotcount\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_geoid[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpop[17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: 0-256 in web mercator\n",
    "# Y: 0-256 in web mercator\n",
    "# 1 <= blockidx <= N inclusive, where N is the number of blocks in the county\n",
    "# 0 <= dotidx < numdots_in_block\n",
    "\n",
    "record_dtype = np.dtype([('x', np.float32), ('y', np.float32), ('block_idx', np.int32), ('dot_idx', np.int32)])\n",
    "record_bytes = record_dtype.itemsize\n",
    "assert record_bytes == 16\n",
    "\n",
    "engine().execute(\"\"\"\n",
    "    -- uncomment drop if arguments to function change\n",
    "    --drop function if exists dotmap_generate_points(geom geometry, block_idx bigint, dotcount integer);\n",
    "    create or replace function dotmap_generate_points(geom geometry, dotcount integer)\n",
    "    returns jsonb\n",
    "    language plpgsql\n",
    "    as $$\n",
    "    declare\n",
    "        rec record;\n",
    "        overage float := 1.2;\n",
    "    begin\n",
    "        while 1 > 0 loop\n",
    "            -- Select first record into rec\n",
    "            SELECT overage, jsonb_agg(ST_X((dp).geom)) as x, jsonb_agg(ST_Y((dp).geom)) as y\n",
    "            INTO rec\n",
    "            FROM (\n",
    "                -- Points from blocks\n",
    "                SELECT \n",
    "                    ST_DumpPoints(\n",
    "                        ST_GeneratePoints(\n",
    "                            -- Transform to Web Mercator 0-256, 0-256\n",
    "                            ----ST_Affine(\n",
    "                            ----    ST_Transform(geom, 3857),\n",
    "                            ----    128/20037508.342789244, 0, \n",
    "                            ----    0, -128/20037508.342789244,\n",
    "                            ----    128, 128),\n",
    "                            geom,\n",
    "                            ceil(dotcount*overage)::int)) AS dp -- TODO: remove seed was ,1\n",
    "                    limit dotcount\n",
    "            ) as points;\n",
    "            if jsonb_array_length(rec.x) = dotcount then\n",
    "                return to_jsonb(rec);\n",
    "            end if;\n",
    "            overage := overage * 1.5;\n",
    "        end loop;\n",
    "    end;$$;\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_shard(shardno: int, first_block_geoid: str, last_block_geoid_incl: str, dest: np.ndarray):\n",
    "    assert len(first_block_geoid) == 15 and len(last_block_geoid_incl) == 15\n",
    "\n",
    "    sys.stdout.write(f\"Query start shard {shardno} {first_block_geoid} to {last_block_geoid_incl}\\n\")\n",
    "    sys.stdout.flush()\n",
    "    records = engine().execute_returning_dicts(f\"\"\"\n",
    "        select indexes.idx as block_idx, dotmap_generate_points(geom, dotcounts.dotcount) as points\n",
    "        from tiger_wgs84.tl_2020_tabblock20 as blocks\n",
    "        join dotmaps.geoids_2020 as indexes on blocks.geoid20 = indexes.geoid\n",
    "        join dotmaps.prototile_2020_dotcount as dotcounts on indexes.idx = dotcounts.idx\n",
    "        where blocks.geoid20 between '{first_block_geoid}' and '{last_block_geoid_incl}z'\n",
    "        order by blocks.geoid20\"\"\")\n",
    "    sys.stdout.write(f\"Query end shard {shardno} {first_block_geoid} to {last_block_geoid_incl}\\n\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    print(records[0])\n",
    "    \n",
    "    first_block_idx = geoid_to_idx[first_block_geoid]\n",
    "    last_block_idx_incl = geoid_to_idx[last_block_geoid_incl]\n",
    "\n",
    "    assert records[0]['block_idx'] == first_block_idx\n",
    "    assert records[-1]['block_idx'] == last_block_idx_incl\n",
    "    assert len(records) == last_block_idx_incl - first_block_idx + 1\n",
    "\n",
    "    total_dots = 0\n",
    "    for record in records:\n",
    "        block_idx = record['block_idx']\n",
    "        if record['points']['overage'] > 3:\n",
    "            sys.stdout.write(f\"Warning: Overage {record['points']['overage']}, block {idx_to_geoid[block_idx]}\\n\")\n",
    "            sys.stdout.flush()\n",
    "        first_dot_idx = cumulative_dots[block_idx - 1]\n",
    "        last_dot_idx_excl = cumulative_dots[block_idx]\n",
    "        ndots = last_dot_idx_excl - first_dot_idx\n",
    "        total_dots += ndots\n",
    "        assert ndots == maxdots[block_idx]\n",
    "        assert ndots == len(record['points']['x'])\n",
    "        dest_slice = dest[first_dot_idx:last_dot_idx_excl]\n",
    "        dest_slice['x'] = record['points']['x']\n",
    "        dest_slice['y'] = record['points']['y']\n",
    "        dest_slice['block_idx'].fill(block_idx)\n",
    "        dest_slice['dot_idx'] = np.arange(len(dest_slice))\n",
    "\n",
    "    #sys.stdout.write(f\"Shard {shardno}: Received {ndots} dots, {first_block_geoid} to {last_block_geoid_incl} (incl), {first_dot_idx} to {last_dot_idx} (excl)\\n\")\n",
    "    sys.stdout.write(f\"Shard {shardno}: Received {total_dots} dots, {first_block_geoid} to {last_block_geoid_incl} (incl)\\n\")\n",
    "    return total_dots\n",
    "\n",
    "# i = 42000\n",
    "# first_block_geoid = geoids_2020[i]\n",
    "# last_block_geoid_incl = geoids_2020[min(i+1000, len(geoids_2020))-1]\n",
    "# process_shard(i // 1000, first_block_geoid, last_block_geoid_incl, np.zeros(10000000, dtype=record_dtype))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This took about an hour on hal21 for about 1.3 million records\n",
    "pool = SimpleThreadPoolExecutor(16)\n",
    "\n",
    "# Create a memory-mapped file\n",
    "prototiledir = \"prototiles.2020\"\n",
    "\n",
    "os.makedirs(prototiledir, exist_ok=True)\n",
    "dest_filename = (f\"{prototiledir}/prototile_all_dots.bin\")\n",
    "\n",
    "override_num_blocks = 20\n",
    "\n",
    "with NamedTemporaryFile(dir=\".\", delete=False) as tmpfile:\n",
    "    all_dots = np.memmap(tmpfile, dtype=record_dtype, mode='w+', shape=ndots)\n",
    "\n",
    "    groupsize = 10000\n",
    "    # Loop through geoids_2020 in groups of groupsize\n",
    "    num_blocks = len(geoids_2020)\n",
    "    num_blocks = override_num_blocks\n",
    "    for i in range(0, num_blocks, groupsize):\n",
    "        first_block_geoid = geoids_2020[i]\n",
    "        last_block_geoid_incl = geoids_2020[min(i+groupsize, num_blocks-1)]\n",
    "        #print(\"submitting \", first_block_geoid, last_block_geoid_incl)\n",
    "        pool.submit(process_shard, i // groupsize, first_block_geoid, last_block_geoid_incl, all_dots)\n",
    "\n",
    "    dotcounts = pool.shutdown(tqdm=tqdm(desc=\"Load dots from postgis\", colour=\"red\"))\n",
    "    print(f\"Created {sum(dotcounts)} dots\")\n",
    "    assert(sum(dotcounts) == ndots)\n",
    "\n",
    "    # Don't forget to flush to ensure data is written to disk\n",
    "    all_dots.flush()\n",
    "    del all_dots\n",
    "\n",
    "filelen = os.stat(tmpfile.name).st_size\n",
    "assert filelen == ndots * record_bytes\n",
    "\n",
    "os.rename(tmpfile.name, dest_filename)\n",
    "\n",
    "sys.stdout.write(f\"Wrote {ndots} dots ({filelen:,} bytes) to {dest_filename}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dots = all_dots[:cumulative_dots[override_num_blocks]]\n",
    "pop_2020 = np.fromfile(\"census2020_block2020/P0010001.2020.float32\", dtype=np.float32)\n",
    "\n",
    "# Load all the generated dots into a geopandas dataframe\n",
    "df = gpd.GeoDataFrame(test_dots, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(test_dots['x'], test_dots['y']))\n",
    "# Select only the dots within population (e.g. 10 dots for a block of 10 population)\n",
    "df = df[df['dot_idx'] < pop_2020[df['block_idx']]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "records = []\n",
    "for geoid in geoids_2020[:override_num_blocks]:\n",
    "    shape = engine().execute_returning_geom(f\"select geom from tiger_wgs84.tl_2020_tabblock20 where geoid20 = '{geoid}'\")\n",
    "    records.append({'geometry': shape, 'geoid': geoid})\n",
    "\n",
    "\n",
    "df = pd.concat([gpd.GeoDataFrame(records, crs=\"EPSG:4326\"), df])\n",
    "\n",
    "#shape = engine().execute_returning_geom(\"select geom from tiger_wgs84.tl_2020_tabblock20 where geoid20 = '010010201001000'\")\n",
    "# add a record to df#\n",
    "#df = pd.concat([df, gpd.GeoDataFrame([{'geometry': shape}])])\n",
    "df.explore()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for tile_path in [\n",
    "    \"prototiles.2020/10/265/413.bin\",\n",
    "    \"prototiles.2020/10/266/413.bin\",\n",
    "    \"prototiles.2020/10/265/414.bin\",\n",
    "    \"prototiles.2020/10/266/414.bin\"\n",
    "]:\n",
    "    prototile = np.fromfile(tile_path, dtype=record_dtype)\n",
    "    dfs.append(pd.DataFrame(prototile))\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df = df[df['block_idx'] <= 20]\n",
    "df\n",
    "\n",
    "def LonLatToWebMercator(lon, lat):\n",
    "    x = (lon + 180.0) * 256.0 / 360.0\n",
    "    y = 128.0 - math.log(math.tan((lat + 90.0) * math.pi / 360.0)) * 128.0 / math.pi\n",
    "    return [x, y]\n",
    "\n",
    "def WebMercatorToLonLat(x,y):\n",
    "    lat = math.atan(math.exp((128.0 - y) * math.pi / 128.0)) * 360.0 / math.pi - 90.0\n",
    "    lon = x * 360.0 / 256.0 - 180.0\n",
    "    return [lon, lat]\n",
    "\n",
    "lonlats = df.apply(lambda row: WebMercatorToLonLat(row['x'], row['y']), axis=1, result_type='expand')\n",
    "\n",
    "df = gpd.GeoDataFrame(df, crs=\"EPSG:4326\", geometry=gpd.points_from_xy(lonlats[0], lonlats[1]))\n",
    "# Select only the dots within population (e.g. 10 dots for a block of 10 population)\n",
    "df = df[df['dot_idx'] < pop_2020[df['block_idx']]]\n",
    "df.explore()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes about 10 minutes on hal21\n",
    "!cp $prototiledir/prototile_all_dots.bin $prototiledir/prototile_all_dots_to_be_shuffled.bin\n",
    "!./shuffle_records $prototiledir/prototile_all_dots_to_be_shuffled.bin && mv $prototiledir/prototile_all_dots_to_be_shuffled.bin $prototiledir/prototile_all_dots_shuffled.bin\n",
    "print(f\"Shuffled dots into {prototiledir}/prototile_all_dots_shuffled.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 2 minutes on hal21\n",
    "\n",
    "subsamples = {\n",
    "    0: 0.001,\n",
    "    1: 0.001,\n",
    "    2: 0.001,\n",
    "    3: 0.001,\n",
    "    4: 0.001,\n",
    "    5: 0.001,\n",
    "    6: 0.004,\n",
    "    7: 0.016,\n",
    "    8: 0.064,\n",
    "    9: 0.256,\n",
    "    10: 1\n",
    "}\n",
    "\n",
    "open(f\"{prototiledir}/subsamples.json\", 'w').write(json.dumps(subsamples))\n",
    "subprocess_check(\"make make_prototile_zoom\", verbose=True)\n",
    "\n",
    "for (z, subsample) in subsamples.items():\n",
    "    subprocess_check(f\"./make_prototile_zoom {prototiledir} {z} {subsample}\", verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsync tiles to hal15\n",
    "# (Paste this into a terminal to enter password interactively)\n",
    "\n",
    "# rsync -av prototiles.2020 hal15.andrew.cmu.edu:/workspace/prototiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
