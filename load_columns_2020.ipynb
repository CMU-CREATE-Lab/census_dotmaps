{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, struct, sys\n",
    "from functools import cache\n",
    "from psql_utils.epsql import Engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tempfile import NamedTemporaryFile\n",
    "from census_utils import canonicalize_census_table_name, canonicalize_census_column_name\n",
    "\n",
    "@cache\n",
    "def engine():\n",
    "    return Engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "dest_dir = \"census2020_block2020\"\n",
    "\n",
    "# Read from block_geoids_2020.json.gz\n",
    "geoids_2020 = json.load(gzip.open('block_geoids_2020.json.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_column_files(table_sql_name: str):\n",
    "    table_name = table_sql_name.split('_')[1]\n",
    "    canonical_table_name = canonicalize_census_table_name(table_name)\n",
    "\n",
    "    prefix = f'{table_name}_'\n",
    "    sql_cols = engine().table_columns(table_sql_name)\n",
    "    col_infos = []\n",
    "    for sql_col in sql_cols:\n",
    "        if sql_col.startswith(prefix):\n",
    "            col = canonicalize_census_column_name(sql_col[len(prefix):])\n",
    "            col_infos.append({\n",
    "                \"sql_col\": sql_col,\n",
    "                \"filename\": f\"{dest_dir}/{canonical_table_name}{col}.2020.float32\"\n",
    "            })\n",
    "    for col_info in col_infos:\n",
    "        if not os.path.exists(col_info[\"filename\"]):\n",
    "            break\n",
    "    else:\n",
    "        print(f\"{table_sql_name} already done, skipping\")\n",
    "        return\n",
    "    \n",
    "    # TODO: using the \"ohio\" package like this, it's about 3x faster (3 min instead of 9 min on hal21)\n",
    "    # df = pd.DataFrame.pg_copy_from(f\"select * from {table_sql_name} order by geoid\", engine(), dtype={\"state\":str, \"county\":str, \"tract\":str, \"block\":str, \"geoid\":str})\n",
    "\n",
    "    df = engine().execute_returning_df(f\"select * from {table_sql_name} order by geoid\")\n",
    "    table_geoids = list(df[\"geoid\"])\n",
    "    assert table_geoids==geoids_2020\n",
    "\n",
    "    for col_info in sorted(col_infos, key=lambda x: x[\"sql_col\"]):\n",
    "        sql_col = col_info[\"sql_col\"]\n",
    "        filename = col_info[\"filename\"]\n",
    "        data = np.array([0] + df[sql_col].tolist(), dtype=np.float32)\n",
    "\n",
    "        # Atomically create dest_filename        \n",
    "        with NamedTemporaryFile('wb', dir=dest_dir, delete=False) as f:\n",
    "            f.write(data.tobytes())\n",
    "        os.rename(f.name, filename)\n",
    "\n",
    "        filelen = os.stat(filename).st_size\n",
    "        assert filelen == (len(df) + 1) * 4\n",
    "        print(\"Wrote %s (%d bytes)\" % (filename, filelen))\n",
    "\n",
    "# NOTE: dec2020pl tables were preliminary, and names conflict with the final dec2020dhc tables\n",
    "tables = [tab for tab in engine().list_tables('census') if tab.startswith('dec2020dhc') and tab.endswith('_block')]\n",
    "tables = sorted(tables)\n",
    "\n",
    "for table_sql_name in tables:\n",
    "    create_table_column_files(f\"census.{table_sql_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
