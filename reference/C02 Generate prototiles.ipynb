{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array, csv, fcntl, glob, json, math, multiprocessing, numpy, os, random, re, shutil\n",
    "import shapely, shapely.wkb, struct, subprocess, sys, threading, urllib2\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (urllib2.urlopen(filename_or_url) if re.match(r'https?:', filename_or_url) else open(filename_or_url)).read()\n",
    "    jsonNb = json.loads(nb)\n",
    "    #check for the modified formatting of Jupyter Notebook v4\n",
    "    if(jsonNb['nbformat'] == 4):\n",
    "        exec '\\n'.join([''.join(cell['source']) for cell in jsonNb['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "    else:\n",
    "        exec '\\n'.join([''.join(cell['input']) for cell in jsonNb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "\n",
    "exec_ipynb('timelapse-utilities.ipynb')\n",
    "\n",
    "set_default_psql_database('census2010')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pyproj\n",
    "except:\n",
    "    !pip install pyproj\n",
    "    import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import shapely\n",
    "except:\n",
    "    !pip install shapely==1.6b2\n",
    "    import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LonLatToPixelXY(lonlat, scale = 1.):\n",
    "    (lon, lat) = lonlat\n",
    "    x = (lon + 180.0) * 256.0 / 360.0\n",
    "    y = 128.0 - math.log(math.tan((lat + 90.0) * math.pi / 360.0)) * 128.0 / math.pi\n",
    "    return [x*scale, y*scale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomPoint(poly):\n",
    "    bbox = poly.bounds\n",
    "    l,b,r,t = bbox\n",
    "    while True:\n",
    "        point = shapely.geometry.point.Point(random.uniform(l,r), random.uniform(t,b))\n",
    "        if point is None:\n",
    "            break\n",
    "        if poly.contains(point):\n",
    "            break\n",
    "    return point.__geo_interface__['coordinates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create census2010_block_idxs mapping each geoid2010 to a sequential integer\n",
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regenerate_census2010_block_idxs = False\n",
    "\n",
    "if regenerate_census2010_block_idxs:\n",
    "    psql(\"\"\"\n",
    "DROP TABLE census2010_block_idxs;\n",
    "CREATE TABLE census2010_block_idxs (blockidx2010 SERIAL PRIMARY KEY,\n",
    "                                   geoid2010 VARCHAR);\n",
    "\n",
    "INSERT INTO census2010_block_idxs \n",
    "SELECT nextval('census2010_block_idxs_blockidx2010_seq'), geoid10\n",
    "FROM tiger2010_census2010_blocks\n",
    "ORDER BY geoid10;\n",
    "CREATE UNIQUE INDEX ON census2010_block_idxs (geoid2010);\n",
    "\"\"\")\n",
    "    \n",
    "psql(\"\\d census2010_block_idxs\")\n",
    "psql(\"SELECT COUNT(*) FROM census2010_block_idxs\")\n",
    "psql(\"SELECT * FROM census2010_block_idxs ORDER BY geoid2010 LIMIT 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psql('SELECT SUM(p001001) FROM sf1_2000_int2010_p001')\n",
    "#psql('SELECT SUM(p001001) FROM sf1_2010_block_p001')\n",
    "psql(\"\"\"\n",
    "SELECT SUM(GREATEST(a.p001001, b.p001001))\n",
    "FROM sf1_2010_block_p001 as a\n",
    "JOIN sf1_2000_int2010_p001 as b USING (geoid2010)\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#psql(\"SELECT COUNT(*) FROM census2010_block_idxs\")\n",
    "#psql(\"SELECT COUNT(*) FROM tiger2010_census2010_blocks\")\n",
    "#psql(\"SELECT COUNT(*) FROM sf1_2010_block_p001\")\n",
    "#psql(\"SELECT COUNT(*) FROM sf1_2010_block_p001 WHERE LEFT(geoid2010,2) != '72'\")\n",
    "#psql(\"SELECT COUNT(*) FROM sf1_2000_int2010_p001\")\n",
    "\n",
    "psql(\"\"\"\n",
    "SELECT COUNT(*) FROM census2010_block_idxs as i\n",
    "JOIN tiger2010_census2010_blocks as shapes ON (i.geoid2010=shapes.geoid10)\n",
    "JOIN sf1_2010_block_p001 as a USING (geoid2010)\n",
    "LEFT JOIN sf1_2000_int2010_p001 as b USING (geoid2010)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psql(\"\"\"\n",
    "SELECT SUM(GREATEST(a.p001001, b.p001001))\n",
    "FROM census2010_block_idxs as i\n",
    "JOIN tiger2010_census2010_blocks as shapes ON (i.geoid2010=shapes.geoid10)\n",
    "JOIN sf1_2010_block_p001 as a USING (geoid2010)\n",
    "JOIN sf1_2000_int2010_p001 as b USING (geoid2010)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute dots for max(2010, 2000 interpolated) block population, by geoid2010\n",
    "----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_psql_parallelism = 8\n",
    "psql_semaphore = multiprocessing.Semaphore(max_psql_parallelism)\n",
    "\n",
    "def query_psql_throttled(query, quiet=False):\n",
    "    psql_semaphore.acquire()\n",
    "    try:\n",
    "        rows = query_psql(query, quiet=quiet)\n",
    "    finally:\n",
    "        psql_semaphore.release()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototiledir = 'prototiles002'\n",
    "!mkdir -p $prototiledir\n",
    "\n",
    "pops = [numpy.load('columncache/census%d_block2010/P0010001.numpy' % y) for y in [1990, 2000, 2010]]\n",
    "caps = numpy.ceil(numpy.maximum(numpy.maximum(pops[0], pops[1]), pops[2]) * 2 + 60).astype(numpy.int32)\n",
    "\n",
    "numpy_atomic_save(prototiledir + '/caps.numpy', caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_format = '<ffii'\n",
    "record_len = struct.calcsize(record_format)\n",
    "\n",
    "def process_shard(start_idx, end_idx_exclusive):\n",
    "    end_idx_inclusive = end_idx_exclusive - 1\n",
    "    shard_filename = 'shards/protomaster-%08d' % start_idx\n",
    "    if os.path.exists(shard_filename):\n",
    "        sys.stdout.write('%s already exists\\n' % shard_filename)\n",
    "        return shard_filename\n",
    "    shard_out = open(shard_filename + '.tmp', 'wb')\n",
    "    query = \"\"\"\n",
    "SELECT i.blockidx2010, shapes.geom\n",
    "FROM (SELECT * FROM census2010_block_idxs WHERE blockidx2010 BETWEEN {start_idx} AND {end_idx_inclusive}) as i\n",
    "JOIN tiger2010_census2010_blocks as shapes ON (i.geoid2010=shapes.geoid10)\n",
    "\"\"\".format(**locals())\n",
    "    rows = query_psql_throttled(query, quiet=True)\n",
    "    begin_time = time.time()\n",
    "\n",
    "    points = []\n",
    "    population = 0\n",
    "    dots = 0\n",
    "    for (blockIdx, geom) in rows:\n",
    "        pop = caps[blockIdx]\n",
    "        population += pop\n",
    "        polygon = shapely.wkb.loads(geom, hex=True)\n",
    "\n",
    "        for i in range(pop):\n",
    "            dots += 1\n",
    "            point = LonLatToPixelXY(randomPoint(polygon))\n",
    "            points.append(struct.pack(record_format, \n",
    "                                      point[0], point[1],\n",
    "                                      blockIdx, i))\n",
    "        if len(points) >= 10000:\n",
    "            shard_out.write(''.join(points))\n",
    "            points = []\n",
    "    shard_out.write(''.join(points))\n",
    "    shard_out.close()\n",
    "    sys.stdout.write(\"Shard {shard_filename} has population {population} and {dots} dots\\n\".format(**locals()))\n",
    "    os.rename(shard_filename + '.tmp', shard_filename)\n",
    "    sys.stdout.write(\"Finished %s with %d rows in %g seconds\\n\" % (shard_filename, len(rows), time.time() - begin_time))\n",
    "    return shard_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_block_idx = query_psql(\"SELECT MAX(blockidx2010) FROM census2010_block_idxs\")[0][0]\n",
    "\n",
    "shard_size = 100000\n",
    "pool = SimpleProcessPoolExecutor(16)\n",
    "\n",
    "results = []\n",
    "\n",
    "print \"Starting shards with maximum index %d\" % max_block_idx\n",
    "\n",
    "if not os.path.exists('shards'):\n",
    "    os.mkdir(\"shards\")\n",
    "\n",
    "for start_idx in range(0, max_block_idx + 1, shard_size):\n",
    "    pool.submit(process_shard, start_idx, start_idx + shard_size)\n",
    "\n",
    "shardfiles = pool.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize point order and write them to binary file\n",
    "---------------------------------------------------\n",
    "\n",
    "Binary file will later be converted to tiles, or, if small enough (i.e. a single state like PA or smaller) could be read directly by the web client page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master = prototiledir + '/master.bin'\n",
    "\n",
    "reread_shard_filenames = True\n",
    "\n",
    "if reread_shard_filenames:\n",
    "    shardfiles = glob.glob('shards/protomaster-????????')\n",
    "    print 'Located %d shard files' % len(shardfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_record_type = [('x','<f4'), ('y','<f4'), ('blockIdx', '<i4'), ('subIdx', '<i4')]\n",
    "\n",
    "if os.path.exists(master):\n",
    "    print '%s already exists' % master\n",
    "else:\n",
    "    npoints = 0\n",
    "    for file in sorted(shardfiles):\n",
    "        nbytes = os.stat(file).st_size\n",
    "        if nbytes % record_len:\n",
    "            raise 'File %s has unexpected length %d' % (file, nbytes)\n",
    "        npoints += nbytes / record_len\n",
    "    print 'From all shards: %d points (%.1fGB)' % (npoints, npoints * record_len / 1e9)\n",
    "\n",
    "    for _ in stopwatch('Reading all shards'):\n",
    "        master_points = numpy.zeros(npoints, dtype=numpy_record_type)\n",
    "        offset = 0\n",
    "        for file in sorted(shardfiles):\n",
    "            shard_points = numpy.fromfile(file, dtype=numpy_record_type)\n",
    "            master_points[offset:offset + len(shard_points)] = shard_points\n",
    "            print 'Placed %d points from %s at offset %d' % (\n",
    "                    len(shard_points), file, offset)\n",
    "            offset += len(shard_points)\n",
    "        assert offset == npoints\n",
    "\n",
    "    for _ in stopwatch('Shuffling points'):\n",
    "        numpy.random.shuffle(master_points)\n",
    "\n",
    "    for _ in stopwatch('Writing %d points to %s' % (npoints, master)):\n",
    "        master_points.tofile(master)\n",
    "        nbytes = os.stat(master).st_size\n",
    "        print 'Master is %.1fGB' % (nbytes / 1e9)\n",
    "        assert npoints * record_len == nbytes\n",
    "        \n",
    "    master_points = None   # allow memory to be reclaimed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate prototiles\n",
    "-------------------\n",
    "\n",
    "master.bin from pre-2021 is located at hal15:uwsgi/dotmaptiles-data/data-visualization-tools/examples/lodes/prototiles002\n",
    "\n",
    "tileserve.py:max_prototile_level is hardcoded to 10, and prototile_subsamples array is also hardcoded, but only used for PNG output (which itself is no longer used?)\n",
    "\n",
    "If we subsample prototiles less, could we then add our own subsample later?  I think we do that currently for level < 5 in tileserve.py:\n",
    "    if z < 5:\n",
    "        # Further subsample the points\n",
    "        subsample = 2.0 ** ((5.0 - z) / 2.0)  # z=4, subsample=2;  z=3, subsample=4 ...\n",
    "        # We're further subsampling the prototile\n",
    "        prototile_subsample /= subsample\n",
    "        incount = int(incount / subsample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too many points to easily fit in RAM;  instead, buffer and write every so often\n",
    "class PrototileWriter:\n",
    "    def __init__(self, z, x, y):\n",
    "        self.filename = \"%s/%d/%d/%d.bin\" % (prototiledir, z, x, y)\n",
    "        if not os.path.exists(os.path.dirname(self.filename)):\n",
    "            os.makedirs(os.path.dirname(self.filename))\n",
    "        open(self.filename + '.tmp', 'w') # truncate file\n",
    "        buffer_len = 256\n",
    "        self.buf = numpy.zeros(buffer_len, dtype=numpy_record_type)\n",
    "        self.size = 0\n",
    "    def write(self, point):\n",
    "        if self.size == len(self.buf):\n",
    "            self.flush()\n",
    "        self.buf[self.size] = point\n",
    "        self.size += 1\n",
    "    def flush(self):\n",
    "        if self.size:\n",
    "            self.buf[0:self.size].tofile(open(self.filename + '.tmp', 'a'))\n",
    "            self.size = 0\n",
    "    def close(self):\n",
    "        self.flush()\n",
    "        os.rename(self.filename + '.tmp', self.filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prototile_zoom(z, subsample):\n",
    "    dim = 2 ** z\n",
    "    with Stopwatch('Building %d prototiles in %s/%d with subsample %g%%' % \n",
    "                   (dim * dim, prototiledir, z, subsample * 100.0)):\n",
    "        tiles = [[PrototileWriter(z, x, y) for y in range(dim)] for x in range(dim)]\n",
    "\n",
    "        num_records = os.stat(master).st_size / record_len\n",
    "    \n",
    "        if subsample < 1:\n",
    "            num_records = int(round(num_records * subsample))\n",
    "                              \n",
    "        with open(master, 'rb') as master_in:\n",
    "            while num_records > 0:\n",
    "                records_to_read = min(num_records, 1024)\n",
    "                bytes = master_in.read(records_to_read * record_len)\n",
    "                assert len(bytes) == records_to_read * record_len\n",
    "                records = numpy.frombuffer(bytes, dtype=numpy_record_type)\n",
    "                assert len(records) == records_to_read\n",
    "                num_records -= records_to_read\n",
    "                for record in records:\n",
    "                    xtile = int(record[0] * dim / 256)\n",
    "                    ytile = int(record[1] * dim / 256)\n",
    "                    tiles[xtile][ytile].write(record)\n",
    "    \n",
    "        for ytile in range(dim):\n",
    "            for xtile in range(dim):\n",
    "                tiles[xtile][ytile].close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom behavior.  bad(1) means brighness step change on level switch.\n",
    "# bad(2) means low res, or brightnness change if we render more levels\n",
    "\n",
    "#       0-5    5-10   10+\n",
    "# PNG   bad(1) good   bad(2)\n",
    "# BIN   good   bad(1) good\n",
    "\n",
    "# PNG\n",
    "#\n",
    "\n",
    "make_prototile_zoom( 0, 0.001) # fully zoomed out\n",
    "make_prototile_zoom( 1, 0.001)\n",
    "make_prototile_zoom( 2, 0.001)\n",
    "make_prototile_zoom( 3, 0.001)\n",
    "make_prototile_zoom( 4, 0.001)\n",
    "make_prototile_zoom( 5, 0.001)\n",
    "make_prototile_zoom( 6, 0.004)\n",
    "make_prototile_zoom( 7, 0.016)\n",
    "make_prototile_zoom( 8, 0.064)\n",
    "make_prototile_zoom( 9, 0.256)\n",
    "make_prototile_zoom(10, 1.0)   # zoomed in to metropolitan area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh prototiles002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def check_point_file(filename):\n",
    "#    n = 0\n",
    "#    with open(filename, 'rb') as file_in:\n",
    "#        print 'Checking %s' % filename\n",
    "#        while True:\n",
    "#            rec = file_in.read(record_len)\n",
    "#            if not rec:\n",
    "#                return\n",
    "#            (x, y, blockIdx, seqWithinBlock) = struct.unpack(record_format, rec)\n",
    "#            if x < 0 or x >= 256:\n",
    "#                print 'Error at %s:%d' % (filename, n)\n",
    "#                print (x, y, blockIdx, seqWithinBlock)\n",
    "#            n += 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
